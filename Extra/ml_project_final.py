# -*- coding: utf-8 -*-
"""ML_Project_Final_ABCDEFG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1StLBdeRQ8lTTcKi5xQHHAELafpG0bgrS
"""

# Import necessary libraries
import os
import numpy as np
import cv2 as cv
from sklearn.preprocessing import LabelEncoder
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense
from keras.models import Sequential
import matplotlib.pyplot as plt
from keras.applications.vgg16 import VGG16
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import zipfile

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define and extract dataset path
zip_file_path = '/content/drive/My Drive/Melanoma_Dataset/melanoma_cancer_dataset.zip'
extract_dir = '/content/melanoma_dataset'

# Extract the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print(f"Dataset extracted to: {extract_dir}")

# Updated dataset path after extraction
dataset_path = extract_dir

# Data preprocessing

def input_data(folder_path, output_data):
    """Importing image data into the output_data list."""
    for dirs in os.listdir(folder_path):
        class_name = dirs  # Subdirectory names represent class labels.
        new_path = os.path.join(folder_path, class_name)
        for img in os.listdir(new_path):
            img_arr = cv.imread(os.path.join(new_path, img), cv.IMREAD_GRAYSCALE)
            resize = cv.resize(img_arr, (128, 128))
            output_data.append([resize, class_name])
    return output_data

train_data = input_data(os.path.join(dataset_path, 'train'), [])
test_data = input_data(os.path.join(dataset_path, 'test'), [])

# Load the data into numpy arrays
train_images = []
train_labels = []
for features, labels in train_data:
    train_images.append(features)
    train_labels.append(labels)

test_images = []
test_labels = []
for features, labels in test_data:
    test_images.append(features)
    test_labels.append(labels)

label_enc = LabelEncoder()  # Encoding the labels
train_labels = label_enc.fit_transform(train_labels)
test_labels = label_enc.transform(test_labels)

train_images = np.array(train_images)
train_labels = np.array(train_labels)
test_images = np.array(test_images)
test_labels = np.array(test_labels)

train_images = train_images / 255.0  # Normalize the image pixels
test_images = test_images / 255.0

train_images = np.expand_dims(train_images, axis=3)  # Add a dimension to the images
test_images = np.expand_dims(test_images, axis=3)

# Print the shape and counts of numpy array
print("Shape of train_images:", train_images.shape)
print("Contents of train_images:", train_images)
print("Shape of test_images:", test_images.shape)
print("Contents of test_images:", test_images)

# Visualize some test images
plt.figure(figsize=(15, 10))
for i in range(25):
    plt.subplot(5, 5, i + 1)
    plt.imshow(test_images[i].squeeze(), cmap='gray')
    plt.title(f"{label_enc.inverse_transform([test_labels[i]])[0]}")
    plt.axis("off")
plt.show()

# Define the CNN model
model1 = Sequential([
    Conv2D(32, (3, 3), input_shape=(128, 128, 1), activation="relu"),
    MaxPool2D((2, 2)),

    Conv2D(64, (3, 3), activation="relu"),
    MaxPool2D((2, 2)),

    Conv2D(128, (3, 3), activation="relu"),
    MaxPool2D((2, 2)),

    Conv2D(256, (3, 3), activation="relu"),
    MaxPool2D((2, 2)),

    Flatten(),
    Dense(256, activation="relu"),
    Dense(1, activation="sigmoid")
])

model1.summary()

# Compile the model
model1.compile(optimizer="adam", loss="binary_crossentropy", metrics=['accuracy'])

# Train the CNN model
history1 = model1.fit(
    train_images, train_labels,
    validation_data=(test_images, test_labels),
    epochs=30
)

# Plot accuracy
plt.plot(history1.history["accuracy"], label="Training Accuracy")
plt.plot(history1.history["val_accuracy"], label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Plot loss
plt.plot(history1.history["loss"], label="Training Loss")
plt.plot(history1.history["val_loss"], label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Predict and evaluate
y_pred1 = model1.predict(test_images)

y_pred1 = (y_pred1 > 0.5).astype(int).flatten()

print(classification_report(test_labels, y_pred1))

sns.heatmap(confusion_matrix(test_labels, y_pred1), fmt='g', annot=True, xticklabels=label_enc.classes_, yticklabels=label_enc.classes_)
plt.title("Confusion Matrix")
plt.show()

# Train and evaluate SVM
from sklearn.svm import SVC

svm_model = SVC(kernel='linear')
svm_model.fit(train_images.reshape(train_images.shape[0], -1), train_labels)

svm_pred = svm_model.predict(test_images.reshape(test_images.shape[0], -1))

print(classification_report(test_labels, svm_pred))
sns.heatmap(confusion_matrix(test_labels, svm_pred), fmt='g', annot=True, xticklabels=label_enc.classes_, yticklabels=label_enc.classes_)
plt.title("Confusion Matrix for SVM")
plt.show()